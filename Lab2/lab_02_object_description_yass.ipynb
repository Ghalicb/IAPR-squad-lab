{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# [IAPR][iapr]: Lab 2 â€’  Object description\n",
    "\n",
    "\n",
    "**Group ID:** 32\n",
    "\n",
    "**Author 1 (sciper):** Julien Daniel Berger (247179)  \n",
    "**Author 2 (sciper):** Ghali Chraibi (262251)   \n",
    "**Author 3 (sciper):** Yasser Haddad (272292)   \n",
    "\n",
    "**Release date:** 26.03.2021  \n",
    "**Due date:** 23.04.2021 \n",
    "\n",
    "\n",
    "## Important notes\n",
    "\n",
    "The lab assignments are designed to teach practical implementation of the topics presented during class well as preparation for the final project, which is a practical project which ties together the topics of the course. \n",
    "\n",
    "As such, in the lab assignments/final project, unless otherwise specified, you may, if you choose, use external functions from image processing/ML libraries like opencv and sklearn as long as there is sufficient explanation in the lab report. For example, you do not need to implement your own edge detector, etc.\n",
    "\n",
    "**! Before handling back the notebook !** rerun the notebook from scratch `Kernel` > `Restart & Run All`\n",
    "\n",
    "\n",
    "[iapr]: https://github.com/LTS5/iapr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tarfile\n",
    "import os\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "from numpy import sign, log10\n",
    "\n",
    "import cv2\n",
    "import skimage.io\n",
    "from skimage.exposure import histogram\n",
    "from skimage import restoration\n",
    "from scipy.signal import convolve2d as conv2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0. Extract relevant data\n",
    "We first need to extract the `lab-02-data.tar.gz` archive.\n",
    "To this end, we use the [tarfile] module from the Python standard library.\n",
    "\n",
    "[tarfile]: https://docs.python.org/3.6/library/tarfile.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_base_path = 'data'\n",
    "data_folder = 'lab-02-data'\n",
    "data_part1 = os.path.join(data_base_path, data_folder, 'part1')\n",
    "data_part2 = os.path.join(data_base_path, data_folder, 'part2')\n",
    "\n",
    "tar_path = os.path.join(data_base_path, data_folder + '.tar.gz')\n",
    "with tarfile.open(tar_path, mode='r:gz') as tar:\n",
    "    tar.extractall(path=data_base_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Part 1\n",
    "In the `lab-02-data/part1` folder, you will find 28x28 grey-scale pictures of handwritten \"0\" and \"1\".\n",
    "These digits have been extracted from MNIST dataset (http://yann.lecun.com/exdb/mnist/).\n",
    "\n",
    "Your goal is to extract, from each of those images, a 2-dimensional feature vector (i.e. 2 features) and to plot them all on a 2D graph.\n",
    "If you have chosen good features, the vectors of the \"0\"'s should nicely cluster in one part of the plane and those of the \"1\"'s in another.\n",
    "\n",
    "Please try:\n",
    "1. Fourier Descriptors (15pts). \n",
    "    1. Implementation (10 pts).\n",
    "    2. Showing invariance to rotation, translation and scaling (5 pts).\n",
    "2. Additional method of your choice (5 pts)\n",
    "\n",
    "\n",
    "**Note:** for the Fourier descriptors, the u_k signal has to be constructed by following the contour point after point. Some pre-processing (image binarization, possibly some Mathematical Morphology) might be useful."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Data visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load(path, digit='0'):\n",
    "    digit_path = os.path.join(path, digit)\n",
    "    digit_names = [nm for nm in os.listdir(digit_path) if '.png' in nm]  # make sure to only load .png\n",
    "    digit_names.sort()  # sort file names\n",
    "    ic = skimage.io.imread_collection([os.path.join(digit_path, nm) for nm in digit_names])\n",
    "    digit_im = skimage.io.concatenate_images(ic)\n",
    "    return digit_im, digit_names\n",
    "                        \n",
    "#  Load zeros and ones\n",
    "zeros_im, zeros_names = load(data_part1, digit='0')\n",
    "ones_im, ones_names = load(data_part1, digit='1')\n",
    "\n",
    "\n",
    "# Plot images\n",
    "fig, axes = plt.subplots(2, len(zeros_im), figsize=(12, 3))\n",
    "for ax, im, nm in zip(axes[0], zeros_im, zeros_names):\n",
    "    ax.imshow(im, cmap='gray')\n",
    "    ax.axis('off')\n",
    "    ax.set_title(nm)\n",
    "for ax, im, nm in zip(axes[1], ones_im, ones_names):\n",
    "    ax.imshow(im, cmap='gray')\n",
    "    ax.axis('off')\n",
    "    ax.set_title(nm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Fourier descriptors (15 pts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.2.A Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fourier_descriptors(img):\n",
    "    # Find the contours of the image\n",
    "    contours, _ = cv2.findContours(\n",
    "        img,\n",
    "        cv2.RETR_EXTERNAL,\n",
    "        cv2.CHAIN_APPROX_NONE)\n",
    "    contour_array = contours[0][:, 0, :]\n",
    "    \n",
    "    # We convert the contour coordinates into a complex representation\n",
    "    contour_complex = np.empty(contour_array.shape[:-1], dtype=complex)\n",
    "    contour_complex.real = contour_array[:, 0]\n",
    "    contour_complex.imag = contour_array[:, 1]\n",
    "    \n",
    "    # We compute the Discrete Fast Fourier Transform of the complex contour and get the descriptors\n",
    "    fourier_descript = np.fft.fft(contour_complex)\n",
    "    \n",
    "    return fourier_descript"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_amplitude(fourier_descript):\n",
    "    amplitudes = []\n",
    "    for f in fourier_descript:\n",
    "        # For each descriptor, get the amplitude of the fourier transform\n",
    "        amplitudes.append(np.sqrt(f.real*f.real + f.imag*f.imag))\n",
    "    \n",
    "    return amplitudes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_two_features(imgs):\n",
    "    f1 = []\n",
    "    f2 = []\n",
    "    for img in imgs:\n",
    "        fourier_descript = fourier_descriptors(img)\n",
    "        amplitudes = get_amplitude(fourier_descript)\n",
    "        # Take only features 1 and 2 (we exclude feature 0)\n",
    "        f1.append(amplitudes[1])\n",
    "        f2.append(amplitudes[2])\n",
    "    \n",
    "    return f1, f2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Without pre-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f1_zeros, f2_zeros = get_two_features(zeros_im)\n",
    "f1_ones, f2_ones = get_two_features(ones_im)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot descriptors\n",
    "fig = plt.figure(1, figsize=(12, 5))\n",
    "\n",
    "plt.scatter(f1_zeros, f2_zeros, label=\"Zero\", color='red')    \n",
    "plt.scatter(f1_ones, f2_ones, label=\"One\", color='blue')    \n",
    "\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With pre-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hist, hist_centers = histogram(ones_im[1])\n",
    "plt.plot(hist)\n",
    "plt.title(\"Histogram of the brain image\")\n",
    "plt.xlabel(\"intensity [grayscale]\")\n",
    "plt.ylabel(\"number of pixels\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  Load zeros and ones\n",
    "zeros_im, zeros_names = load(data_part1, digit='0')\n",
    "ones_im, ones_names = load(data_part1, digit='1')\n",
    "\n",
    "#img = zeros_im[0]\n",
    "#img = ones_im[0]\n",
    "#im = cv2.erode(img, kernel, iterations=1)\n",
    "#im = cv2.morphologyEx(img, cv2.MORPH_OPEN, kernel)\n",
    "#im = cv2.dilate(img,kernel,iterations = 1)\n",
    "\n",
    "def thresholding(img, range1, range2):\n",
    "    # Apply a threshold to keep only the pixels withing the indicated range\n",
    "    thresh = cv2.inRange(img, range1, range2)\n",
    "    \n",
    "    return thresh\n",
    "    \n",
    "\n",
    "def restore(img):\n",
    "    psf = np.ones((5, 5)) / 25\n",
    "    t = conv2(img, psf, 'same')\n",
    "    t += 0.1 * t.std() * np.random.standard_normal(t.shape)\n",
    "    deconvolved, _ = restoration.unsupervised_wiener(t, psf)\n",
    "    \n",
    "    return deconvolved\n",
    "\n",
    "\n",
    "def transform_opening(img, kernel_size=(3,3)):\n",
    "    kernel = np.ones(kernel_size, np.uint8)\n",
    "    t = cv2.morphologyEx(img, cv2.MORPH_OPEN, kernel)\n",
    "    \n",
    "    return t\n",
    "\n",
    "def transform_closing(img, kernel_size=(3,3)):\n",
    "    kernel = np.ones(kernel_size, np.uint8)\n",
    "    closing = cv2.morphologyEx(img, cv2.MORPH_CLOSE, kernel)\n",
    "    \n",
    "    return closing\n",
    "\n",
    "def transform_high_pass(img):\n",
    "    kernel = np.array([[0.0, -1.0, 0.0], \n",
    "                       [-1.0, 5.0, -1.0],\n",
    "                       [0.0, -1.0, 0.0]])\n",
    "\n",
    "    #kernel = kernel/(np.sum(kernel) if np.sum(kernel)!=0 else 1)\n",
    "    t = cv2.filter2D(img, -1, kernel)\n",
    "    \n",
    "    return t\n",
    "\n",
    "preprocessed_zeros = [transform_closing(thresholding(img, 127, 255), kernel_size=(2,2)) for img in zeros_im]\n",
    "preprocessed_ones = [transform_closing(thresholding(img, 127, 255), kernel_size=(2,2)) for img in ones_im]\n",
    "\n",
    "# Plot images\n",
    "fig, axes = plt.subplots(4, len(zeros_im), figsize=(20, 4))\n",
    "for ax, im in zip(axes[0], zeros_im):\n",
    "    ax.imshow(im, cmap='gray')\n",
    "    ax.axis('off')\n",
    "    \n",
    "for ax, im, nm in zip(axes[1], preprocessed_zeros, zeros_names):\n",
    "    ax.imshow(im, cmap='gray')\n",
    "    ax.axis('off')\n",
    "\n",
    "for ax, im in zip(axes[2], ones_im):\n",
    "    ax.imshow(im, cmap='gray')\n",
    "    ax.axis('off')\n",
    "    \n",
    "for ax, im, nm in zip(axes[3], preprocessed_ones, ones_names):\n",
    "    ax.imshow(im, cmap='gray')\n",
    "    ax.axis('off')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f1_zeros, f2_zeros = get_two_features(preprocessed_zeros)\n",
    "f1_ones, f2_ones = get_two_features(preprocessed_ones)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot descriptors\n",
    "fig = plt.figure(1, figsize=(12, 5))\n",
    "\n",
    "plt.scatter(f1_zeros, f2_zeros, label=\"Zero\", color='red')    \n",
    "plt.scatter(f1_ones, f2_ones, label=\"One\", color='blue')    \n",
    "\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 Additional method (5 pts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hu moments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Moments of an image allow to capture information about the shape of an object in a binary image. However, they are not invariant in *translation*, *scaling* and *rotation*. **Normalized central moments** are translation and scaling invariant, but not rotation invariant. \n",
    "\n",
    "This is where **Hu moments** come into play. They are based on a combination of normalized central moments and are translation, scaling and rotation invariant. They consist of 7 moments, the first 6 of which are also *reflection invariant*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Hu moments** are within a large range, and to be able to compare them and efficiently use them together, we use the following Log transform : $H_i = -\\text{sign}(h_i) * \\log_{10} |h_i|$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_hu_moments(img):\n",
    "    \"\"\" \n",
    "    Computes and returns the transformed Hu moments of an image.\n",
    "    \n",
    "    The function first finds Hu moments and then applies a Log transform. \n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    img : np.ndarray (MxM)\n",
    "        A 2D image\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    np.ndarray (7, 1)\n",
    "        The transformed Hu moments of the image\n",
    "    \"\"\"\n",
    "    # Calculate Moments\n",
    "    moments = cv2.moments(img)\n",
    "    # Calculate Hu Moments\n",
    "    huMoments = cv2.HuMoments(moments)\n",
    "    # Apply log transform for each moment\n",
    "    for i in range(0, 7):\n",
    "        huMoments[i] = -1 * sign(huMoments[i]) * log10(abs(huMoments[i]))\n",
    "        \n",
    "    return huMoments\n",
    "\n",
    "def get_features_hu_moments(imgs, feature1_idx, feature2_idx):\n",
    "    \"\"\" \n",
    "    Computes and returns 2 Hu moments for each image \n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    imgs : list(np.ndarray) (Nx(MxM))\n",
    "        A list of 2D images\n",
    "    \n",
    "    feature1_idx : int\n",
    "        Index of the\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    (list(float), list(float))\n",
    "        The lists containing the requested Hu moments for each image\n",
    "    \"\"\"\n",
    "    f1 = []\n",
    "    f2 = []\n",
    "    \n",
    "    for img in imgs:\n",
    "        huMoments = get_hu_moments(img)\n",
    "        f1.append(huMoments[feature1_idx])\n",
    "        f2.append(huMoments[feature2_idx])\n",
    "        \n",
    "    return f1, f2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To know which 2 moments to choose, we plot the combination of all moments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot all combination of moments\n",
    "fig, axes = plt.subplots(6, 6, figsize=(24, 24))\n",
    "\n",
    "for i in range(7):\n",
    "    for j in range(i+1, 7):\n",
    "    \n",
    "        f1_zeros, f2_zeros = get_features_hu_moments(zeros_im, i, j)\n",
    "        f1_ones, f2_ones = get_features_hu_moments(ones_im, i, j)\n",
    "\n",
    "        axes[i, j-1].scatter(f1_zeros, f2_zeros, label=\"Zero\", color='red')    \n",
    "        axes[i, j-1].scatter(f1_ones, f2_ones, label=\"One\", color='blue') \n",
    "        axes[i, j-1].set_xlabel(\"Moment \" + str(i+1), fontsize=14)\n",
    "        axes[i, j-1].set_ylabel(\"Moment \" +str(j+1), fontsize=14)\n",
    "\n",
    "plt.tick_params(labelcolor=\"none\", bottom=False, left=False)\n",
    "#plt.subplots_adjust(bottom = 0.01, left=0.001)\n",
    "plt.tight_layout()\n",
    "\n",
    "plt.suptitle(\"Plot of all combinations of features\", y=1.03, fontsize=20)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We notice that the 2d vector of moment 2 and 3 gives the most linearly separable populations in the plot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f1_zeros, f2_zeros = get_features_hu_moments(zeros_im, 1, 2)\n",
    "f1_ones, f2_ones = get_features_hu_moments(ones_im, 1, 2)\n",
    "\n",
    "fig = plt.figure(1, figsize=(12, 5))\n",
    "\n",
    "plt.scatter(f1_zeros, f2_zeros, label=\"Zero\", color='red')    \n",
    "plt.scatter(f1_ones, f2_ones, label=\"One\", color='blue')   \n",
    "\n",
    "plt.xlabel(\"2nd Moment\")\n",
    "plt.ylabel(\"3rd Moment\")\n",
    "\n",
    "plt.title(\"Plot of the second and third Hu moments\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Part 2\n",
    "The `lab-02-data/part2` folder contains grey-scale pictures of handwritten \"2\" and \"3\".\n",
    "Extract the same feature (typically 2 Fourier descriptors) as in part 1 also on these images and plot them on the same graph as the features of the \"0\" and \"1\".\n",
    "Is it possible to discriminate all these 4 digits with a 2-dimensional feature vector?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Data visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  Load twos and threes\n",
    "twos_im, twos_names = load(data_part2, digit='2')\n",
    "threes_im, threes_names = load(data_part2, digit='3')\n",
    "\n",
    "# Plot images\n",
    "fig, axes = plt.subplots(2, len(twos_im), figsize=(12, 3))\n",
    "for ax, im, nm in zip(axes[0], twos_im, twos_names):\n",
    "    ax.imshow(im, cmap='gray')\n",
    "    ax.axis('off')\n",
    "    ax.set_title(nm)\n",
    "for ax, im, nm in zip(axes[1], threes_im, threes_names):\n",
    "    ax.imshow(im, cmap='gray')\n",
    "    ax.axis('off')\n",
    "    ax.set_title(nm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Fourier descriptors - 4 digits (10 pts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add your implementation and discussion"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
