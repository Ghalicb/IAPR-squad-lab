{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# [IAPR][iapr]: Project\n",
    "\n",
    "\n",
    "**Group ID:** 32\n",
    "\n",
    "**Author 1 (sciper):** Ghali CHRAIBI (262251)  \n",
    "**Author 2 (sciper):** Yann Yasser HADDAD (272292)   \n",
    "**Author 3 (sciper):** Julien BERGER (247179)   \n",
    "\n",
    "**Release date:** 07.05.2021  \n",
    "**Due date:** 03.06.2021 (23h59)\n",
    "\n",
    "\n",
    "## Important notes\n",
    "\n",
    "The lab assignments are designed to teach practical implementation of the topics presented during class as well as preparation for the final project, which is a practical project which ties together the topics of the course. \n",
    "\n",
    "As such, in the lab assignments/final project, unless otherwise specified, you may, if you choose, use external functions from image processing/ML libraries like opencv and sklearn as long as there is sufficient explanation in the lab report. For example, you do not need to implement your own edge detector, etc.\n",
    "\n",
    "**! Before handling back the notebook !** rerun the notebook from scratch `Kernel` > `Restart & Run All`\n",
    "\n",
    "\n",
    "[iapr]: https://github.com/LTS5/iapr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 0. Introduction\n",
    "\n",
    "An anonymous researcher that we will name Lann Yecun is convinced that the MNIST dataset still has great potential. He decides to create a playing card game based on MNIST digits and different figures. The game uses a standard 52 card deck which is composed of four French suits/colours: clubs (&#9827;), diamonds (&#9830;), hearts (&#9829;) and spades (&#9824;). Each suit includes 10 digit cards (from 0 to 9) and 3 figures (Jack-J, Queen-Q, and King-K). Here is an example of the 13 spade cards with their name.\n",
    "\n",
    "\n",
    "<img src=\"data/media/example_cards.png\">\n",
    "\n",
    "\n",
    "We can find the same arrangement of cards for the clubs, diamonds, and hearts. \n",
    "\n",
    "\n",
    "## 1. Rules\n",
    "\n",
    "\n",
    "### 1.1 Standard\n",
    "\n",
    "The rules are based on the simple battle card game. The goal of the game is to win as many points as possible. Each turn, the 4 players play a card in front of them. As displayed in the example below. The rules are the following:\n",
    "\n",
    "- The cards are ranked in the following order : **0 < 1 < 2 < 3 < 4 < 5 < 6 < 7 < 8 < 9 < J < Q < K**.\n",
    "- The player with the highest-ranked card wins the round and obtains **1 point**. \n",
    "- If the highest-ranked card is the same for multiple players we call it a draw and all winners get **1 points**. \n",
    "- In this configuration, we **do not** take into account the suits. The game only rely on the card ranks. \n",
    "- The game lasts 13 rounds. After the last round, the winner is the player that has the largest number of points. \n",
    "- In the example below Player 1 wins the round with his Queen ( 0 < 8 < J < **Q**).\n",
    "\n",
    "If two or more players have the same number of points they share the victory.\n",
    "\n",
    "### 1.2 Advanced\n",
    "\n",
    "The advanced rules take into account the suits. \n",
    "\n",
    "- At the beginning of **each round** a random player is designated as the **dealer**. The dealer places a green token with the letter *D* next to him (player 1 in the example below).\n",
    "- Only the cards that belong to the same suit as the one of the dealer are considered valid. In the example below, only Player 4 is competing with Player 1 as spade was selected by the dealer (e.i., Player 1). Player 2 and 3 are out for this round. Player 1 wins the round and **1 point** with the Queen ( 0&#9824; < **Q&#9824;**).\n",
    "- There cannot be any draw between the players as they are not any card duplicates.\n",
    "- We use the same system as the standard method to count the points.\n",
    "\n",
    "\n",
    "<img src=\"data/media/example_round.jpg\">\n",
    "\n",
    "\n",
    "### 1.3 Notes\n",
    "\n",
    "- The orientation of the card is linked to the position of the player around the table. For instance, to read the card of the 3rd player you will have to rotate it by 180Â°.\n",
    "- The **digits** always **face** the players around the table. The figures can have random orientations.\n",
    "- Player 1 **always** seats south of the table. The players are **always** ordered counter-clockwise as in the example. \n",
    "- The dealers can change between the rounds and games.\n",
    "- Some cards **might** apear multiple times per game.\n",
    "- Pictures are always taken from rougthly the same altitude.\n",
    "- The digits from the training set **would not** be the same as the one of the testing set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 2. Data\n",
    "\n",
    "You will be given the images of 7 games that were played ([download link](https://drive.google.com/drive/folders/1fEy27wnJsUJPRsEEomzoAtP56s-7HFtk?usp=sharing)). The data are composed of:\n",
    "   - 7 folder named after the games (game1 to game7).\n",
    "   - Each game includes 13 ordered images (1st to 13th round).\n",
    "   - Each game includes a csv file with the ground truth of the game. The first row list the players (P1 to P4) as well as the dealer (D). The following rows represent the rounds (1 to 13). We represent the card played with 2 character as $AB$ where $A \\in [0-9, J, Q, K]$ is the rank of the card and $B \\in [C, D, H, S]$ is the suit. For example, QS means \"(Q)ueen of (S)pade\" and 0D means \"(0) of (D)iamond\". The dealer is represented by the ID of the player (e.g. P1 -> 1).\n",
    "   \n",
    "You are free to use external datasets such as the original MNIST train set that you used in lab 3."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 3. Your Tasks\n",
    "\n",
    "Your task is to ready yourself for the final evaluation. The day of the exam we will give you a new folder with a new game. ! The digits on the cards **differ** from the one of the traning set. When given a new data folder with 13 images your should be able to:\n",
    "\n",
    "**Task 0**\n",
    "   - Plot an overlay for each round image that shows your detections and classification. You can for example plot bounding boxes around the cards/dealer token and add a text overlay with the name of the classes.\n",
    "\n",
    "**Task 1**\n",
    "   - (a) Predict the **rank** of the card played by each player at each round (Standard rules).\n",
    "   - (b) Predict the **number of points** of each player according to **Standard** rules\n",
    " \n",
    "**Task 2**\n",
    "   - (a) Detect which player is the selected **dealer** for each round.\n",
    "   - (b) Predict the **rank** and the **suit** of the card played by each player at each round (Advanced rules).\n",
    "   - (c) Predict the **number of points** of each player according to **Advanced** rules\n",
    "\n",
    "---\n",
    "\n",
    "**Before the exam (until 03.06.21 at 23h59)**\n",
    "   - Create a zipped folder named **group_xx.zip** that you upload on moodle (xx being your group number).\n",
    "   - Include a **runnable** code (Jupyter Notebook and external files) and your presentation in the zip folder.\n",
    "   \n",
    "**The day of the exam (04.06.21)**\n",
    "   - You will be given a **new folder** with 13 images (rounds) and but **no ground truth** (csv file).\n",
    "   - We will ask you to run your pipeline in **realtime** and to send us your prediction of task 1 and 2 that you obtain with the function **print_results**. \n",
    "   - On our side we will compute the perfomance of your classification algorithm. \n",
    "   - To evaluate your method we will use the **evaluate_game** function presented below. To understand how the provided functions work please read the documentation of the functions in **utils.py**.\n",
    "   - **Please make sure your function returns the proper data format to avoid points penalty the day of the exam**. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports & Constants\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import skimage.io\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils import data\n",
    "\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "from MNISTDataset import MNISTDataset\n",
    "from CNN import CNN\n",
    "\n",
    "from utils import *\n",
    "from MNIST_utils import extract_data, extract_labels\n",
    "from ML_pipeline import augment_data, train_loop, test_loop, predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "NB_CLASS_RANK = 13\n",
    "NB_CLASS_SUIT = 4\n",
    "\n",
    "TRAIN_RANK_MODEL = True\n",
    "SAVE_RANK_MODEL = True\n",
    "TRAIN_SUIT_MODEL = True\n",
    "SAVE_SUIT_MODEL = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data loading\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = 'data/train_games'\n",
    "train_game_count = 7\n",
    "game_round_count = 13\n",
    "\n",
    "dict_data = {}\n",
    "\n",
    "for i in range(1, train_game_count+1):\n",
    "    # For each game, store the path of the csv and the image of each round\n",
    "    dict_data[f'game{i}'] = {}\n",
    "    dict_data[f'game{i}']['url'] = train_data + f'/game{i}'\n",
    "    dict_data[f'game{i}']['csv'] = train_data + f'/game{i}.csv'\n",
    "    \n",
    "    for j in range(1, game_round_count+1):\n",
    "        dict_data[f'game{i}'][f'round{j}'] = skimage.io.imread(train_data + f'/game{i}/{j}.jpg')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Rank detection model\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load MNIST data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_shape = (28, 28)\n",
    "train_set_size = 60000\n",
    "test_set_size = 10000\n",
    "\n",
    "mnist_folder = os.path.join('data', 'MNIST')\n",
    "\n",
    "mnist_train_images_path = os.path.join(mnist_folder, 'train-images-idx3-ubyte.gz')\n",
    "mnist_train_labels_path = os.path.join(mnist_folder, 'train-labels-idx1-ubyte.gz')\n",
    "mnist_test_images_path = os.path.join(mnist_folder, 't10k-images-idx3-ubyte.gz')\n",
    "mnist_test_labels_path = os.path.join(mnist_folder, 't10k-labels-idx1-ubyte.gz')\n",
    "\n",
    "mnist_train_images = extract_data(mnist_train_images_path, image_shape, train_set_size)\n",
    "mnist_test_images = extract_data(mnist_test_images_path, image_shape, test_set_size)\n",
    "mnist_train_labels = extract_labels(mnist_train_labels_path, train_set_size)\n",
    "mnist_test_labels = extract_labels(mnist_test_labels_path, test_set_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load segmented ranks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "filepath_segmented_rank = os.path.join('data', 'segmented_data', 'all_games_ranks.pickle')\n",
    "df_segmented_rank, df_segmented_numbers, df_segmented_figures = load_segmented_rank(filepath_segmented_rank)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform_figure = transforms.Compose(\n",
    "    [\n",
    "        transforms.ToPILImage(),\n",
    "        transforms.RandomAffine(degrees=25, scale=(0.7, 1), shear=25),\n",
    "        transforms.ToTensor(),\n",
    "    ]\n",
    ")\n",
    "\n",
    "transform_mnist = transforms.Compose(\n",
    "    [\n",
    "        transforms.ToPILImage(),\n",
    "        transforms.RandomAffine(degrees=25, scale=(0.8, 1.1), shear=20),\n",
    "        transforms.ToTensor(),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "figures_label = {'J': 10, 'Q': 11, 'K': 12}\n",
    "\n",
    "train_augmented_figs, train_augmented_figs_labels, val_augmented_figs, \\\n",
    "        val_augmented_figs_labels, test_figs, test_figs_labels = \\\n",
    "            get_train_val_test_figures(df_segmented_figures, figures_label, transform_figure)\n",
    "\n",
    "train_augmented_mnist = augment_data(mnist_train_images, transform_mnist)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create DataLoaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters for the DataLoaders\n",
    "params = {'batch_size': 128,\n",
    "          'shuffle': True}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creation of a train/test dataset & dataloader\n",
    "train_merged_images = np.concatenate((mnist_train_images, train_augmented_mnist, train_augmented_figs))\n",
    "train_merged_labels = np.concatenate((mnist_train_labels, mnist_train_labels, train_augmented_figs_labels))\n",
    "\n",
    "val_merged_images = np.concatenate((mnist_test_images, val_augmented_figs))\n",
    "val_merged_labels = np.concatenate((mnist_test_labels, val_augmented_figs_labels)) \n",
    "\n",
    "ds_train_rank = MNISTDataset(train_merged_images, train_merged_labels, transform=transforms.ToTensor())\n",
    "dl_train_rank = data.DataLoader(ds_train_rank, **params)\n",
    "\n",
    "ds_val_rank = MNISTDataset(val_merged_images, val_merged_labels, transform=transforms.ToTensor())\n",
    "dl_val_rank = data.DataLoader(ds_val_rank, **params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "segmented_numbers = np.array([v for v in df_segmented_numbers.image.apply(img_as_ubyte)])\n",
    "\n",
    "ds_test_numbers = MNISTDataset(segmented_numbers, df_segmented_numbers['rank'].to_numpy().astype(np.int64),\n",
    "                               transform=transforms.ToTensor())\n",
    "dl_test_numbers = data.DataLoader(ds_test_numbers, **params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_test_figs = MNISTDataset(test_figs, test_figs_labels.astype(np.int64), transform=transforms.ToTensor())\n",
    "dl_test_figs = data.DataLoader(ds_test_figs, **params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build & Train the model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1\n",
      "-------------------------------\n",
      "It 1055/1055:\tLoss train: 0.04867, Accuracy train: 97.56%%\n",
      "Test Error:\n",
      "\tAvg loss: 0.00037, Accuracy: 98.36%\n",
      "\n",
      "Epoch 2\n",
      "-------------------------------\n",
      "It 1055/1055:\tLoss train: 0.08209, Accuracy train: 97.56%%\n",
      "Test Error:\n",
      "\tAvg loss: 0.00049, Accuracy: 98.08%\n",
      "\n",
      "Epoch 3\n",
      "-------------------------------\n",
      "It 1055/1055:\tLoss train: 0.00184, Accuracy train: 100.00%\n",
      "Test Error:\n",
      "\tAvg loss: 0.00076, Accuracy: 97.70%\n",
      "\n",
      "Epoch 4\n",
      "-------------------------------\n",
      "It 1055/1055:\tLoss train: 0.00812, Accuracy train: 100.00%\n",
      "Test Error:\n",
      "\tAvg loss: 0.00072, Accuracy: 97.91%\n",
      "\n",
      "Epoch 5\n",
      "-------------------------------\n",
      "It 1055/1055:\tLoss train: 0.03904, Accuracy train: 97.56%%\n",
      "Test Error:\n",
      "\tAvg loss: 0.00075, Accuracy: 98.04%\n",
      "\n",
      "Epoch 6\n",
      "-------------------------------\n",
      "It 1055/1055:\tLoss train: 0.09817, Accuracy train: 98.78%%\n",
      "Test Error:\n",
      "\tAvg loss: 0.00066, Accuracy: 98.47%\n",
      "\n",
      "Epoch 7\n",
      "-------------------------------\n",
      "It 1055/1055:\tLoss train: 0.06091, Accuracy train: 98.78%%\n",
      "Test Error:\n",
      "\tAvg loss: 0.00085, Accuracy: 97.97%\n",
      "\n"
     ]
    }
   ],
   "source": [
    "if TRAIN_RANK_MODEL:\n",
    "    rank_model = CNN()\n",
    "\n",
    "    # Loss function\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "    # Optimizer\n",
    "    learning_rate = 1e-3\n",
    "    opt = torch.optim.Adam(rank_model.parameters(), lr=learning_rate)\n",
    "\n",
    "    epochs = 7\n",
    "\n",
    "    # Train & test our model until convergence\n",
    "    for e in range(epochs):\n",
    "        print(f\"Epoch {e+1}\\n-------------------------------\")\n",
    "        train_loop(dl_train_rank, rank_model, criterion, opt)\n",
    "        test_loop(dl_val_rank, rank_model, criterion)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Error:\n",
      "\tAvg loss: 0.01094, Accuracy: 82.08%\n",
      "\n"
     ]
    }
   ],
   "source": [
    "if TRAIN_RANK_MODEL:\n",
    "    test_loop(dl_test_numbers, rank_model, criterion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Error:\n",
      "\tAvg loss: 0.02205, Accuracy: 94.44%\n",
      "\n"
     ]
    }
   ],
   "source": [
    "if TRAIN_RANK_MODEL:\n",
    "    test_loop(dl_test_figs, rank_model, criterion)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "if SAVE_RANK_MODEL:\n",
    "    models_dir = os.path.join('data', 'models')\n",
    "\n",
    "    if not os.path.isdir(models_dir):\n",
    "        os.mkdir(models_dir)\n",
    "        \n",
    "    torch.save(rank_model.state_dict(), os.path.join(models_dir, \"model_rank.pt\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 8,  0,  5,  9, 10,  3,  2,  3,  8,  7,  4,  0,  6,  3,  2,  8,  8,  3,\n",
       "         4,  7,  2,  6,  9,  7, 10,  5,  5,  1,  7,  6, 10,  4,  2,  6,  8,  2,\n",
       "         1,  5,  0,  2,  2,  7, 10,  6,  8,  4,  8,  2,  5,  1,  5,  0,  0,  3,\n",
       "         4,  1,  0,  7,  2,  0,  2,  1,  6,  4,  8,  2,  1,  8,  0,  3,  9,  5,\n",
       "         5,  5,  3,  6,  6,  3,  7,  6,  0,  1,  1,  7,  6,  3,  2,  3,  7,  4,\n",
       "         2,  6,  9,  7,  0,  8,  3,  0,  0,  5,  1,  3,  4,  0, 10,  2,  5,  2,\n",
       "         7,  6,  5,  6,  5,  4,  2,  8,  8,  8,  8,  3,  2,  6,  3,  9,  3,  5,\n",
       "         7,  7,  3,  8,  8,  8,  4,  2,  1,  0, 10,  0,  8,  7,  7,  0,  1,  2,\n",
       "         0,  5,  2,  5,  4,  5,  2,  8,  4,  6,  3,  5,  6,  6,  7,  2,  2,  9,\n",
       "         0,  2,  3,  7,  6,  8,  4,  7,  6,  3,  5,  5,  3,  6,  6,  3,  8,  5,\n",
       "         1,  5,  2,  8,  4,  3,  4,  7,  5,  0, 10,  8,  7,  0,  1, 10,  6,  0,\n",
       "         1,  7,  5,  4,  0,  7,  6, 10,  8,  5,  5,  3,  2,  1,  2,  9, 10,  6,\n",
       "         2,  7,  3,  1,  0,  2,  3,  1,  4,  1,  6,  7,  5,  2,  0,  2,  3,  8,\n",
       "         8,  0,  7,  0,  8,  0,  0,  7,  8,  0,  0,  8,  8,  4,  1,  0,  2,  7,\n",
       "        10,  3,  2,  7,  6,  4,  3,  6,  5,  1,  2,  5,  3,  1,  7, 10,  5,  5,\n",
       "         8,  7,  3,  9,  8,  4,  5,  2,  6])"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### TODO do something with this TODO ###\n",
    "predict(segmented_numbers, NB_CLASS_RANK, CNN, \"data/models/model_rank.pt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Suit detection model\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load segmented ranks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "filepath_suits = os.path.join('data', 'segmented_data', 'all_games_suits.pickle')\n",
    "\n",
    "segmented_suits = pd.read_pickle(filepath_suits)\n",
    "segmented_suits_values = segmented_suits.values\n",
    "imgs = [v[1] for v in segmented_suits_values]\n",
    "ranks = [v[0][0] for v in segmented_suits_values]\n",
    "suits = [v[0][1] for v in segmented_suits_values]\n",
    "\n",
    "df_suits = pd.DataFrame({\"rank\": ranks, \"suit\": suits, \"image\": imgs})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform_suit = transforms.Compose(\n",
    "    [\n",
    "        transforms.ToPILImage(),\n",
    "        transforms.RandomAffine(degrees=25, scale=(0.7, 1), shear=25),\n",
    "        transforms.ToTensor(),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "suits_labels = {'C': 0, 'D': 1, 'H': 2, 'S': 3}\n",
    "\n",
    "train_augmented_suits, train_augmented_suits_labels, val_augmented_suits, val_augmented_suits_labels, \\\n",
    "            test_suits, test_suits_labels = \\\n",
    "            get_train_val_test_suits(df_suits, suits_labels, transform_suit)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create DataLoaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creation of a train/test dataset & dataloader \n",
    "ds_train_suit = MNISTDataset(train_augmented_suits, train_augmented_suits_labels, transform=transforms.ToTensor())\n",
    "dl_train_suit = data.DataLoader(ds_train_suit, **params)\n",
    "\n",
    "ds_val_suit = MNISTDataset(val_augmented_suits, val_augmented_suits_labels, transform=transforms.ToTensor())\n",
    "dl_val_suit = data.DataLoader(ds_val_suit, **params)\n",
    "\n",
    "ds_test_suit = MNISTDataset(test_suits, test_suits_labels, transform=transforms.ToTensor())\n",
    "dl_test_suit = data.DataLoader(ds_test_suit, **params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build & Train the model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1\n",
      "-------------------------------\n",
      "It 30/30:\tLoss train: 0.26477, Accuracy train: 95.29%\n",
      "Test Error:\n",
      "\tAvg loss: 0.00198, Accuracy: 91.35%\n",
      "\n",
      "Epoch 2\n",
      "-------------------------------\n",
      "It 30/30:\tLoss train: 0.13751, Accuracy train: 97.65%%\n",
      "Test Error:\n",
      "\tAvg loss: 0.00032, Accuracy: 99.19%\n",
      "\n",
      "Epoch 3\n",
      "-------------------------------\n",
      "It 30/30:\tLoss train: 0.04598, Accuracy train: 98.82%%\n",
      "Test Error:\n",
      "\tAvg loss: 0.00014, Accuracy: 99.73%\n",
      "\n",
      "Epoch 4\n",
      "-------------------------------\n",
      "It 30/30:\tLoss train: 0.08945, Accuracy train: 97.65%%\n",
      "Test Error:\n",
      "\tAvg loss: 0.00013, Accuracy: 100.00%\n",
      "\n",
      "Epoch 5\n",
      "-------------------------------\n",
      "It 30/30:\tLoss train: 0.02843, Accuracy train: 100.00%\n",
      "Test Error:\n",
      "\tAvg loss: 0.00020, Accuracy: 99.73%\n",
      "\n"
     ]
    }
   ],
   "source": [
    "if TRAIN_SUIT_MODEL:\n",
    "    model = CNN(nb_classes=4)\n",
    "\n",
    "    # Loss function\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "    # Optimizer\n",
    "    learning_rate = 1e-3\n",
    "    opt = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "    epochs = 5\n",
    "\n",
    "    # Train & test our model until convergence\n",
    "    for e in range(epochs):\n",
    "        print(f\"Epoch {e+1}\\n-------------------------------\")\n",
    "        train_loop(dl_train_suit, model, criterion, opt)\n",
    "        test_loop(dl_val_suit, model, criterion)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Error:\n",
      "\tAvg loss: 0.00058, Accuracy: 98.64%\n",
      "\n"
     ]
    }
   ],
   "source": [
    "if TRAIN_SUIT_MODEL:\n",
    "    test_loop(dl_test_suit, model, criterion)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "if SAVE_SUIT_MODEL:\n",
    "    models_dir = os.path.join('data', 'models')\n",
    "\n",
    "    if not os.path.isdir(models_dir):\n",
    "        os.mkdir(models_dir)\n",
    "        \n",
    "    torch.save(model.state_dict(), os.path.join(models_dir, \"model_suits.pt\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "        2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2,\n",
       "        2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3,\n",
       "        3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3,\n",
       "        3, 3, 3])"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### TODO do something with this TODO ###\n",
    "predict(test_suits, NB_CLASS_SUIT, CNN, \"data/models/model_suits.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### 4.1 Example Final results\n",
    "\n",
    "Example of output you **should** provide the day of the final exam."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creates dummy predictions (toy exmaple)\n",
    "pred_rank = np.array([\"0D\"]*4*13).reshape((13, 4)) # Everyone played the \"0 of spade\".\n",
    "pred_dealer = [1]*13                # List of players selected as dealer for each round\n",
    "pred_pts_stand = [0,0,0,13]         # Player 4 won 13 points with standard rules.\n",
    "pred_pts_advan = [0,0,8,7]          # Player 3 and 4 won 8 and 7 points with adv, rules respectively.\n",
    "\n",
    "print_results(\n",
    "    rank_colour=pred_rank, \n",
    "    dealer=pred_dealer, \n",
    "    pts_standard=pred_pts_stand,\n",
    "    pts_advanced=pred_pts_advan,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### 4.2 Example Accuracy\n",
    "\n",
    "Example of code you can use to validate the performance of your model. Be careful the day of the exam you will not have access to the ground truth of the predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load ground truth from game 1\n",
    "cgt = pd.read_csv('train_games/game1/game1.csv', index_col=0)\n",
    "cgt_rank = cgt[['P1', 'P2', 'P3', 'P4']].values\n",
    "\n",
    "# Compute accuracy of prediction\n",
    "acc_standard = evaluate_game(pred_rank, cgt_rank, mode_advanced=False)\n",
    "acc_advanced = evaluate_game(pred_rank, cgt_rank, mode_advanced=True)\n",
    "print(\"Your model accuracy is: Standard={:.3f}, Advanced={:.3f}\".format(acc_standard, acc_advanced))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
