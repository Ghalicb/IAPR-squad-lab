{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "average-retrieval",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "\n",
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import cv2\n",
    "import skimage.io\n",
    "\n",
    "\n",
    "from skimage.color import label2rgb, gray2rgb, rgb2gray, rgb2hsv\n",
    "\n",
    "from skimage.transform import resize, rotate, rescale\n",
    "\n",
    "from skimage.util import img_as_ubyte, crop\n",
    "\n",
    "from skimage.filters import (laplace, prewitt, sobel, roberts, median,\n",
    "gaussian, threshold_otsu, threshold_multiotsu,  difference_of_gaussians,\n",
    "threshold_isodata, threshold_mean, threshold_yen, threshold_sauvola,\n",
    "threshold_niblack, threshold_triangle, threshold_li, threshold_local)\n",
    "\n",
    "from skimage.segmentation import clear_border\n",
    "\n",
    "from skimage.measure import label, regionprops, regionprops_table, find_contours\n",
    "\n",
    "from skimage.morphology import (disk, square, closing, binary_opening,\n",
    "                        binary_closing, opening, binary_dilation, binary_erosion, remove_small_objects)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "diagnostic-producer",
   "metadata": {},
   "source": [
    "# Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "genetic-venice",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_path = os.path.join('data', 'train_games')\n",
    "train_game_count = 7\n",
    "game_round_count = 13\n",
    "\n",
    "dict_data = {}\n",
    "\n",
    "for i in range(1, train_game_count+1):\n",
    "    # For each game, store the path of the csv and the image of each round\n",
    "    dict_data[f'game{i}'] = {}\n",
    "    dict_data[f'game{i}']['url'] = train_data_path + f'/game{i}'\n",
    "    dict_data[f'game{i}']['csv'] = train_data_path + f'/game{i}.csv'\n",
    "    \n",
    "    for j in range(1, game_round_count+1):\n",
    "        dict_data[f'game{i}'][f'round{j}'] = skimage.io.imread(train_data_path + f'/game{i}/{j}.jpg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "persistent-apache",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 1, figsize=(8,8))\n",
    "ax.imshow(dict_data['game3']['round7'])\n",
    "ax.set_title('Game1 Round1')\n",
    "ax.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dirty-stephen",
   "metadata": {},
   "source": [
    "# Segmentation function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecological-institute",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Input: coloured image of the round\n",
    "#Output: DataFrame containing all segmented objects (cards + dealer + random objects)\n",
    "#with geometrical properties\n",
    "def find_potential_objects_in_original_round_image(coloured_image):\n",
    "    hsv_img = rgb2hsv(coloured_image) \n",
    "    hsv_img = img_as_ubyte(hsv_img) # Convert 0-1 to 0-255 \n",
    "    \n",
    "    thres_brightness = threshold_multiotsu(hsv_img[:,:,2]).max() # threshold for the \"value\" channel\n",
    "    \n",
    "    # HSV image is filtered on the 3 channels:\n",
    "    # H: [145-200] white colour , S: [0-255] , V: [threshold-255] high values\n",
    "    filtered_im = cv2.inRange(hsv_img, np.array([145,0,thres_brightness]), np.array([200,255,255])) # Binary filtered image\n",
    "    \n",
    "    labeled_im = label(filtered_im, background=None, connectivity=filtered_im.ndim) # Find objects using region growing and labeling\n",
    "    labeled_im = remove_small_objects(labeled_im, min_size=30000) # Remove objects smaller than the dealer and the cards\n",
    "    labeled_im = median(labeled_im, square(15)) # Remove noise\n",
    "    \n",
    "    # Dictionnary that stores information for each segmented objects \n",
    "    props_objs = regionprops_table(labeled_im, properties=(\n",
    "                                                  \"label\",\n",
    "                                                  \"area\",\n",
    "                                                  \"filled_area\",\n",
    "                                                  \"major_axis_length\",\n",
    "                                                  \"minor_axis_length\",\n",
    "                                                  'centroid',\n",
    "                                                  \"slice\",\n",
    "                                                  \"image\"\n",
    "                                                   ))\n",
    "    df_objs = pd.DataFrame(props_objs)\n",
    "    df_objs[\"labeled_im\"] = 0 # random value\n",
    "    df_objs[\"labeled_im\"] = df_objs[\"labeled_im\"].apply(lambda x: labeled_im)\n",
    "    \n",
    "    \n",
    "    return df_objs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "greatest-pottery",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Input: DataFrame containing all the potential objects \n",
    "#Output: DataFrame with 4 objects corresponding to the 4 cards (unordered)\n",
    "\n",
    "def select_cards_from_potential_objects(df_objs): \n",
    "    \n",
    "    # Features differentiating cards from the rest of the objects\n",
    "    # These are reference values obtained by exploring the dataset\n",
    "    C_REF_MAJOR_AXIS_MEAN = 838\n",
    "    C_REF_MAJOR_AXIS_STD = 16\n",
    "    C_REF_MINOR_AXIS_MEAN = 542\n",
    "    C_REF_MINOR_AXIS_STD = 23\n",
    "    \n",
    "    cards_feature1_series = (df_objs[\"major_axis_length\"]-C_REF_MAJOR_AXIS_MEAN)/C_REF_MAJOR_AXIS_STD # Normalized feature 1 \n",
    "    cards_feature2_series = (df_objs[\"minor_axis_length\"]-C_REF_MINOR_AXIS_MEAN)/C_REF_MINOR_AXIS_STD # Normalized feature 2\n",
    "    \n",
    "    # Euclidian distance corresponding to the similarity between an object and the reference card.\n",
    "    # The smaller the distance, the greater the similarity. Here the 4 most similar objects are kept.\n",
    "    cards_similarity_distances_series = (cards_feature1_series**2+cards_feature2_series**2)**0.5 # Euclidian distance\n",
    "    df_objs[\"card_similarity_measure\"] = cards_similarity_distances_series\n",
    "    \n",
    "    return df_objs.nsmallest(4, \"card_similarity_measure\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:iapr] *",
   "language": "python",
   "name": "conda-env-iapr-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
